{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29f4d79",
   "metadata": {},
   "source": [
    "\n",
    "In the wrangle_act.ipynb notebook, the Weratedogs twitter data archive was used for our research purpose. Provided was the enhanced twitter archive that contained over 2000+ of tweets with their ratings as well as the image predictions tsv file that was provided using a neural network project. \n",
    "\n",
    "The first step taken was to load these different sources of data programmatically into the notebook. Next was querying the twitter API using tweepy that provided additional information for the dataset such as retweet_count, and the  favorite_count. To do this, we needed to build an app, generate user secrets, which we successfully did. \n",
    "\n",
    "The road blocker was getting the application for app developer approved by twitter's developer team. A mail needed to be sent to them explaining the purpose for the app and some back and forth with the team. Alternatively, we could have used the json.txt file to run the project without waiting for the approval. The approval was an experience worth having eventually. For all intents and purposes, further exploration can be performed in the future with this newfound privilege (smiles).\n",
    "\n",
    "Back to the project, we ran the query using the query id's provided in the twitter archive and wrote the data into a 'tweet_json.txt' file from which we used the readlines method to read the data line by line into a pandas dataframe. \n",
    "\n",
    "Next was assessing the data. To do this, we had to make copies of the original dataframes we had using the copy method. In assessing the copied dataframes, we were able to identify some data quality issues and tidiness issues. We then proceeded to the cleaning stage that involved the define-code-test loop which we ran on both datasets bringing them both to high quality dataframes. We began cleaning the data by identifying and eliminating retweets(non-original tweets).\n",
    "\n",
    "Following the cleaning stage was storing the data. To do this, we merged the two datasets - the twitter archive dataset and the tweepy dataset which produced a master dataset with over 20 columns using the pandas.merge method with the how = \"inner\" argument amongst the other arguments. \n",
    "\n",
    "After the cleaning, came the visualization and analyses. During this stage, we identified some key findings and produced some visualizations using the matplotlib library which was well documented.\n",
    "\n",
    "And finally, here we are, at the Reporting section. Making a detailed report of what was done, (efforts put into the project).\n",
    "So in summary, we followed the steps outlined at the beginning of the project, which are as follows:\n",
    "* Gather the data\n",
    "* Assess the data\n",
    "* Clean the data\n",
    "* Store the data\n",
    "* Analyse and visualize the data\n",
    "* Report the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5234532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
